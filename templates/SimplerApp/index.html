{% extends 'SimplerApp/base.html' %}
{% load static %}
{% block scripts_block %}
	<script src="{% static 'js/index.js' %}"></script>
{% endblock %}

{% block css_block %}
	<link href="{% static 'css/index.css' %}" rel="stylesheet" />
	<link href="{% static 'css/highlight.css' %}" rel="stylesheet" />
{% endblock %}

{% block body_block %}

<div style='padding-bottom:30px;'>
{% if user.is_authenticated %}
{% else %}
	<div style="text-align:center;"><h1><!--<a href='/register/'>Register Here</a> or --><a href="/login/">Login.</a></h1></div>
{% endif %}
{% if user.is_authenticated %}
{% autoescape off %}

	<h1>Reinforcement Learning</h1>

	<h2>CS670, IIT Madras</h2>

	<div id="contentind" style="font-size:9pt;">
		<span>Jump to, read and ask questions:</span>
		<span>&nbsp;&nbsp;&nbsp;<a href="/sutton/">The Problem</a></span>
			<span>&nbsp;&nbsp;&nbsp;<a href="/sutton1/">Elementary Solution Methods</a></span>
				<span>&nbsp;&nbsp;&nbsp;<a href="/sutton2/">A Unified View</a></span>
				</div>

	<div id="coursenavwrap">

		<span id="cfeed" class="coursenav">COURSE FEED</span>
		<span id="linksnav" class="coursenav">COURSE CONTENT</span>
			<span id='overview' class="coursenav">OVERVIEW</span>

		</div>
<div id="linkspage" style="display:none; padding:20px; padding-top:0px;">
	<h1>Reinforcement Learning: An Introduction</h1>
	<li><h2>&nbsp;&nbsp;&nbsp;<a href="/sutton/">The Problem</a></h2>
	<li><h2>&nbsp;&nbsp;&nbsp;<a href="/sutton1/">Elementary Solution Methods</a></h2>
	<li><h2>&nbsp;&nbsp;&nbsp;<a href="/sutton2/">A Unified View</a></h2>
</div>
	<div id="afeed">

	{% for a in act %}

	{% if a.answer %}

	<div class="afeedel ansact">

		<div class="activity" data-actid="{{a.id}}">
			<span class="getup" data="{{a.user.id}}"><a href="javascript:;">{{a.user.username}}</a></span><span class="notiftext"> added an answer.</span>
			<div class="anscontandqueswrap contandqueswrap viewcont" data-coef="{{a.question.highlight.highlight_parent.coeficient}}" data-q="{{a.question.highlight.highlight_parent.question}}">
				<div class="quespluscont">
					{% if a.question.highlight.highlight_parent.coeficient >= 1 %}
					<div class="prevques">{{a.question.highlight.highlight_parent.answerto}}</div>
					{% endif %}
				<div class="context">{{a.question.highlight.highlight_parent.answer}}</div>
			</div>
				<div class="floatques" data-hp="{{a.question.highlight.id}}">
					{{a.question.question}}
					<div class="triangle"></div>
				</div>
		</div>
			<div class="activityques" data-id="{{a.question.id}}" data-highlight="{{a.question.highlight.id}}" data-parent="{{a.question.highlight.highlight_parent.id}}">
				<a href="javascript:;">{{a.question.question}}</a>
				</div>
				<div class="activityans" data-ansid="{{a.answer.id}}">
					{{a.answer.answer}}
					<div class="upview">
						<div class="author" style="float:left; margin-right:10px;">Answer By <br/><a href="javascript:;" class="getup" data="{{a.answer.writer.id}}">{{a.answer.writer.username}}</a></div>
						<div class="upnumup"><span><button class="btn btn-primary up"></button></span> <span style="color:grey;">&bull;</span> <span class="upnum" style="color:grey;">{{a.answer.upvoters.all|length}}</span></div>
					</div>
					</div>
					{% if a in newact %}<span class="newact">NEW ACTIVITY</span>{% endif %}
		</div>
	</div>

	{% else %}

	<div class="afeedel quesact">

		<div class="activity" data-actid="{{a.id}}">
			<span class="getup" data="{{a.user.id}}"><a href="javascript:;">{{a.user.username}}</a></span><span class="notiftext">
				{% if a.question.assignment == 'y' %} assigned a question. {% else %}
				has this question. {% endif %}</span>
			<div class="contandqueswrap quescontandqueswrap">
					<div class="quespluscont">
					{% if a.question.highlight.highlight_parent.coeficient >= 1 %}<div class="prevques">{{a.question.highlight.highlight_parent.answerto}}</div>{% endif %}
					<div class="context">{{a.question.highlight.highlight_parent.answer}}</div>
				</div>
					<div class="activityques floatques" data-id="{{a.question.id}}" data-parent="{{a.question.highlight.highlight_parent.id}}" data-hp="{{a.question.highlight.id}}">
						<a class="queslink" href="javascript:;">{{a.question.question}}</a>
						<div class="triangle"></div>
						<div class="upview">
							<button class="btn btn-primary up"></button>
						</div>
					</div>
			</div>
			{% if a in newact %}<span class="newact">NEW ACTIVITY</span>{% endif %}
			{% if a.question.assignment == 'y' %} <span class="assign">Assignment</span> {% endif %}
		</div>
	</div>

	{% endif %}
	{% endfor %}

</div>

<div id="ocont" style="font-size:10pt;">

	<h3>Purpose</h3>

	This course will provide a comprehensive introduction to reinforcement learning, a powerful approach to learning from interaction to achieve goals in stochastic and incompletely-known environments. Reinforcement learning has adapted key ideas from machine learning, operations research, control theory, psychology, and neuroscience to produce some strikingly successful applications. The focus is on algorithms for learning what actions to take, and when to take them, so as to optimize long-term performance. This may involve sacrificing immediate reward to obtain greater reward in the long-term or just to obtain more information about the environment. The course will cover Markov decision processes, dynamic programming, temporal-difference learning, policy gradient reinforcement learning methods, Monte Carlo reinforcement learning methods, eligibility traces, the role of function approximation, hierarchical reinforcement learning approaches, and the integration of learning and planning.

	Prerequisites: Interest in learning approaches to artificial intelligence; basic probability theory; computer programming ability. Please talk with the instructor if you want to take the course but have doubts about your qualifications.

	<h3>Syllabus</h3>

	The Reinforcement Learning problem: evaluative feedback, non-associative learning, Rewards and returns, Markov Decision Processes, Value functions, optimality and approximation
	Dynamic programming: value iteration, policy iteration, asynchronous DP, generalized policy iteration
	Monte-Carlo methods: policy evaluation, roll outs, on policy and off policy learning, importance sampling
	Temporal Difference learning: TD prediction, Optimality of TD(0), SARSA, Q-learning, R-learning, Games and after states
	Eligibility traces: n-step TD prediction, TD(lambda), forward and backward views, Q(lambda), SARSA(lambda), replacing traces and accumulating traces
	Function Approximation: Value prediction, gradient descent methods, linear function approximation, ANN based function approximation, lazy learning, instability issues
	Policy Gradient methods: non-associative learning - REINFORCE algorithm, exact gradient methods, estimating gradients, approximate policy gradient algorithms, actor-critic methods
	Planning and Learning: Model based learning and planning, prioritized sweeping, Dyna, heuristic search, trajectory sampling, E^3 algorithm
	Hierarchical RL: MAXQ framework, Options framework, HAM framework, airport algorithm, hierarchical policy gradient
	Case studies: Elevator dispatching, Samuel's checker player, TD-gammon, Acrobot, Helicopter piloting

	<h3>Text Books</h3>

	1. <a href="/sutton/">R. S. Sutton and A. G. Barto: "Reinforcement Learning: An Introduction". Cambridge, MA: MIT Press, 1998.</a>

	<h3>References</h3>

	1. "Neuro-dynamic programming". Dimitri P. Bertsikas and John N. Tsitsiklis.
	2. "Learning Automata - An Introduction". Kumpati S. Narendra and M. A. L. Thathachar.</div>

{% endautoescape %}
{% endif %}

</div>
<div id="empty"></div>
{% endblock %}
